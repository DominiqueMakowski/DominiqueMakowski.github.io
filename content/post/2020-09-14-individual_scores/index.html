---
title: "How to extract individual scores from repeated measures"
subtitle: "We compare different methods (individual models, Bayesian models with informative priors, random effects from mixed models) to extract individual scores from repeated measures tasks."
summary: "We compare different methods (individual models, Bayesian models with informative priors, random effects from mixed models) to extract individual scores from repeated measures tasks."
draft: false
featured: false
authors:
- Dominique Makowski
date: 2020-09-14
categories:
- R
- Statistics
tags:
- Statistics
- R
- Neuropsychology
- Bayesian
- Individual scores
image:
  caption: 'Absolute difference between true individual scores and the scores estimated via different methods.'
  placement: 1
---



<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>Many psychology fields require to extract individual scores, i.e., point-estimates (<em>i.e.</em>, a single value) for a participant/patient, to be used as an index of something and later interpreted or re-used in further statistical analyses. This single index is often derived from several “trials”. For instance, the reaction times in the condition A (let’s say, the baseline) will be <strong>averaged</strong> together, and the same will be done with the condition B. Finally, the difference between these two means will be used an the <strong>individual score</strong> for a given participant.</p>
<p>However, we can intuitively feel that we <strong>lose a lot of information</strong> when averaging these scores. Do we deal appropriately with the variability related to individuals, conditions, or the noise aggravated by potential outliers? This is especially important when working with a limited amount of trials.</p>
<p>With the advent of recent computational advances, new easy-to-implement alternatives emerge. For instance, <strong>one can “model” the effects at an individual level</strong> (e.g., the simplest case, for the two conditions paradigm described above, would be a linear regression with the condition as a unique predictor), and use the <strong>parameters</strong> of each model as individual scores (e.g., the “slope” coefficient of the effect of the manipulation), rather than the raw mean. This opens up the possibility of including covariates and take into account other sources of known variability, which could lead to better estimates.</p>
<p>However, individual models are also sensitive to outliers and noise. Thus, another possibility is to <strong>model the effects at the population level</strong> and, <em>at the same time</em>, at the individual level. This can be achieved by modelling the participants as a <strong>random factor in a mixed model</strong>. In this case, the individual estimates might benefit from the population estimates. In other words, the effects at the population level will “constrain” or “guide” the estimation at an individual level to potentially limit extreme parameters.</p>
<p>Unfortunately, the above method requires to have all the data at hand, to be able to fit the population model. This is often not the case in on-going acquisition, or in neuropsychological contexts, in which the practitioners simply acquire data for one patient, and have to compute individual scores, without having access to the detailed population data. Thus, an in-between alternative could make use of <strong>Bayesian models</strong>, in which the population effects (for instance, the mean effect of the condition) could be entered as an informative <strong>prior</strong> in the individual models to, again, “guide” the estimation at an individual level and hopefully limit the impact of noise or outliers observations.</p>
<p>In this post, the aim is to compare these 4 methods (basic individual model - equivalent to using the raw mean, population model, individual model with informative priors) in recovering the “true” effects using a simulated dataset.</p>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<div id="generate-data" class="section level4">
<h4>Generate Data</h4>
<p>We generate several datasets in which we manipulate the number of participants, in which the score of interest is the effect of a manipulation as compared to a baseline condition. 20 trials per condition will be generated with a known “true” effect (the centre of the distribution from which the data is generated). Gaussian noise of varying standard deviation will be added to create a natural variability (See the functions’ definition below).</p>
<pre class="r"><code>library(tidyverse)
library(easystats)

data &lt;- get_data(n_participants=1000, n_trials=20)
results &lt;- get_results(data)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="individual.png" alt="*Example of a dataset containing 20 participants (shown with different colors). As can be seen, we introduced modulations in the inter- and intra- individual variability.*" width="1575" />
<p class="caption">
Figure 1: <em>Example of a dataset containing 20 participants (shown with different colors). As can be seen, we introduced modulations in the inter- and intra- individual variability.</em>
</p>
</div>
<p>We will then compare the scores obtained by each method to the “true” score of each participant by substracting them from one another. As such, for each method, we obtain the absolute “distance” from the true score.</p>
</div>
<div id="fit-model" class="section level4">
<h4>Fit model</h4>
<p>Contrast analysis will be applied to compare the different methods together.</p>
<pre class="r"><code>model &lt;- lm(Diff_Abs ~ Method, data=results)

modelbased::estimate_contrasts(model) %&gt;%
  arrange(Difference) %&gt;%
  mutate(Level1 = stringr::str_remove(Level1, &quot;Diff_&quot;),
         Level2 = stringr::str_remove(Level2, &quot;Diff_&quot;)) %&gt;% 
  select(Level1, Level2, Difference, CI_low, CI_high, p)</code></pre>
<pre><code>## Level1                 |                 Level2 | Difference |            CI |      p
## -------------------------------------------------------------------------------------
## IndividualModel_Priors |        PopulationModel |  -1.85e-03 | [-0.01, 0.01] | &gt; .999
## IndividualModel_Freq   |        PopulationModel |   1.70e-03 | [-0.01, 0.01] | &gt; .999
## IndividualModel_Freq   | IndividualModel_Priors |   3.55e-03 | [-0.01, 0.01] | &gt; .999</code></pre>
</div>
<div id="visualize-the-results" class="section level4">
<h4>Visualize the results</h4>
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="featured.png" alt="*Average accuracy of the different methods (the closest to 0 the better).*" width="2250" />
<p class="caption">
Figure 2: <em>Average accuracy of the different methods (the closest to 0 the better).</em>
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-7"></span>
<img src="n_participants.png" alt="*Accuracy depending on the number of total participants in the dataset.*" width="2250" />
<p class="caption">
Figure 3: <em>Accuracy depending on the number of total participants in the dataset.</em>
</p>
</div>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>Though not significantly different, it seems that <strong>raw basic estimates</strong> (that rely only on the individual data) <strong>perform consistently worse than the population model or individual models informed by priors</strong>, especially for small datasets (between 10 and 100 participants) - though again, the difference is tiny in our simulated dataset. In the absence of the whole population dataset, it seems that using individual Bayesian model with informative priors (derived from the population model) is a safe alternative.</p>
</div>
<div id="functions" class="section level3">
<h3>Functions</h3>
<pre class="r"><code>library(tidyverse)
library(easystats)
library(rstanarm)
library(ggdist)


# Get data ----------------------------------------------------------------

get_data &lt;- function(n_participants = 10, n_trials = 20, d = 1.5, var = 3, noise = 0.1) {
  scores_baseline &lt;- rnorm(n_participants, 0, 1)
  scores_condition &lt;- rnorm(n_participants, d, 1)
  variances &lt;- rbeta(n_participants, 2, 8)
  variances &lt;- 0.1 + variances * (var / max(variances)) # Rescale
  noise_sd &lt;- abs(rnorm(n_participants, 0, noise))

  data &lt;- data.frame()
  for (i in 1:n_participants) {
    a &lt;- rnorm(n_trials, scores_baseline[i], variances[i])
    b &lt;- rnorm(n_trials, scores_condition[i], variances[i])
    a &lt;- a + rnorm(n_trials, 0, noise_sd[i]) # Add noise
    b &lt;- b + rnorm(n_trials, 0, noise_sd[i]) # Add noise
    data &lt;- rbind(data, data.frame(
      &quot;Participant&quot; = sprintf(&quot;S%02d&quot;, i),
      &quot;Y&quot; = c(a, b),
      &quot;Score_True&quot; = rep(c(scores_baseline[i], scores_condition[i]), each = n_trials),
      &quot;Condition&quot; = rep(c(&quot;Baseline&quot;, &quot;Manipulation&quot;), each = n_trials)
    ))
  }
  data
}



# Visualize data -----------------------------------------------------------

p &lt;- get_data(n_participants = 20) %&gt;%
  group_by(Participant, Condition) %&gt;%
  mutate(mean = mean(Y)) %&gt;%
  ggplot(aes(y = Y, x = Condition, fill = Participant, color = Participant, group = Participant)) +
  geom_line(aes(y = mean), position = position_dodge(width = 0.66)) +
  ggdist::stat_eye(point_interval = ggdist::mean_hdi, alpha = 0.66, position = position_dodge(width = 0.66), .width = c(0.95)) +
  ylab(&quot;Score&quot;) +
  theme_modern() +
  theme(legend.position = &quot;none&quot;)
ggsave(&quot;individual.png&quot;, p, width = 7, height = 7, dpi = 450)


# Get results -------------------------------------------------------------


get_results &lt;- function(data) {

  # Raw method ----

  results &lt;- data %&gt;%
    group_by(Participant, Condition) %&gt;%
    summarise_all(mean) %&gt;%
    rename(&quot;Score_Raw&quot; = &quot;Y&quot;) %&gt;%
    arrange(Condition, Participant) %&gt;%
    ungroup()


  # Population model ----

  model &lt;- lme4::lmer(Y ~ Condition + (1 + Condition | Participant), data = data)

  fixed &lt;- insight::get_parameters(model, effects = &quot;fixed&quot;)$Estimate
  random &lt;- insight::get_parameters(model, effects = &quot;random&quot;)$Participant

  # Transform coefs into scores
  pop_baseline &lt;- random[, 1] + fixed[1]
  pop_manipulation &lt;- pop_baseline + random[, 2] + fixed[2]

  results$Score_PopulationModel &lt;- c(pop_baseline, pop_manipulation)


  # Individual model ----

  individual_model_data &lt;- data.frame()
  for (participant in unique(data$Participant)) {
    cat(&quot;.&quot;) # Print progress

    dat &lt;- data[data$Participant == participant, ]

    # Frequentist
    model1 &lt;- lm(Y ~ Condition, data = dat)
    nopriors &lt;- parameters::parameters(model1)$Coefficient

    # Bayesian without priors
    # model2 &lt;- stan_glm(Y ~ Condition, data = dat, refresh = 0)
    # bayes &lt;- parameters::parameters(model2)$Median

    # Bayesian with Priors
    model3 &lt;- stan_glm(Y ~ Condition,
      data = dat,
      refresh = 0,
      prior = normal(fixed[1]),
      prior_intercept = normal(fixed[2])
    )
    priors &lt;- parameters::parameters(model3)$Median

    individual_model_data &lt;- rbind(
      individual_model_data,
      data.frame(
        &quot;Participant&quot; = c(participant, participant),
        &quot;Condition&quot; = c(&quot;Baseline&quot;, &quot;Manipulation&quot;),
        &quot;Score_IndividualModel&quot; = c(nopriors[1], nopriors[1] + nopriors[2]),
        # &quot;Score_IndividualModel_Bayes&quot; = c(bayes[1], bayes[1] + bayes[2]),
        &quot;Score_IndividualModel_Priors&quot; = c(priors[1], priors[1] + priors[2])
      )
    )
  }

  results &lt;- merge(results, individual_model_data)


  # Clean output ----

  diff &lt;- results %&gt;%
    mutate(
      # Diff_Raw = Score_True - Score_Raw,
      Diff_PopulationModel = Score_True - Score_PopulationModel,
      Diff_IndividualModel = Score_True - Score_IndividualModel,
      # Diff_IndividualModel_Bayes = Score_True - Score_IndividualModel_Bayes,
      Diff_IndividualModel_Priors = Score_True - Score_IndividualModel_Priors
    ) %&gt;%
    select(Participant, Condition, starts_with(&quot;Diff&quot;)) %&gt;%
    pivot_longer(starts_with(&quot;Diff&quot;), names_to = &quot;Method&quot;, values_to = &quot;Diff&quot;) %&gt;%
    mutate(Diff_Abs = abs(Diff))

  diff
}



# Analysis ----------------------------------------------------------------
results &lt;- data.frame()
for(n in seq.int(10, 300, length.out = 10)){
  data &lt;- get_data(n_participants = round(n), n_trials = 20)
  rez &lt;- get_results(data) %&gt;%
    select(-Participant) %&gt;%
    group_by(Condition, Method) %&gt;%
    summarise_all(mean) %&gt;%
    mutate(n_Participants = n,
           Method = as.factor(Method),
           Dataset=paste0(&quot;Dataset&quot;, round(n, 2)))
  results &lt;- rbind(results, rez)

  print(n) # Print progress
}

# model &lt;- mgcv::gam(Diff_Abs ~ Method + s(n_Participants, by = Method), data = results)
model &lt;- lm(Diff_Abs ~ Method * poly(n_Participants, 3), data = results)

parameters::parameters(model)


contrasts &lt;- modelbased::estimate_contrasts(model) %&gt;%
  arrange(Difference) %&gt;%
  mutate(
    Level1 = stringr::str_remove(Level1, &quot;Diff_&quot;),
    Level2 = stringr::str_remove(Level2, &quot;Diff_&quot;)
  ) %&gt;%
  select(Level1, Level2, Difference, CI_low, CI_high, p)




# Visualize results ---------------------------------------------------------
p &lt;- modelbased::estimate_means(model) %&gt;%
  arrange(Mean) %&gt;%
  mutate(
    Method = stringr::str_remove(Method, &quot;Diff_&quot;),
    Method = factor(Method, levels = Method)
  )  %&gt;%
  ggplot(aes(x = Method, y = Mean, color = Method)) +
  geom_line(aes(group = 1)) +
  geom_pointrange(aes(ymin = CI_low, ymax = CI_high), size = 1) +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_material()
ggsave(&quot;featured.png&quot;, p, width = 10, height = 6, dpi = 450)



p &lt;- modelbased::estimate_relation(model) %&gt;%
  mutate(Method = stringr::str_remove(Method, &quot;Diff_&quot;))  %&gt;%
  ggplot(aes(x = n_Participants, y = Predicted)) +
  geom_point(data=mutate(results, Method = stringr::str_remove(Method, &quot;Diff_&quot;)),
             aes(y=Diff_Abs, color = Method)) +
  geom_ribbon(aes(ymin=CI_low, ymax=CI_high, fill=Method), alpha=0.1) +
  geom_line(aes(color = Method), size=1) +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_material() +
  scale_fill_material()
ggsave(&quot;n_participants.png&quot;, p, width = 10, height = 6, dpi = 450)


# Save results ------------------------------------------------------------


d &lt;- list(&quot;results&quot; = results, &quot;model&quot; = model, &quot;contrasts&quot; = contrasts)
save(d, file = &quot;data.Rdata&quot;)</code></pre>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p><sub>You can reference this post as follows:</sub></p>
<p><sub>- Makowski, D. (2020, September 14). How to extract individual scores from repeated measures. Retrieved from <a href="https://dominiquemakowski.github.io/post/individual_scores/" class="uri">https://dominiquemakowski.github.io/post/individual_scores/</a></sub></p>
<hr />
<p><em>Thanks for reading! Do not hesitate to share this post, and leave a comment below</em> :hugs:</p>
<p>🐦 <em>And don’t forget to join me on Twitter</em> <span class="citation">[@Dom_Makowski]</span>(<a href="https://twitter.com/Dom_Makowski" class="uri">https://twitter.com/Dom_Makowski</a>)</p>
</div>
