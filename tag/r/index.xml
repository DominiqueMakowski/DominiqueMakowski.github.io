<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Dr Dominique Makowski</title>
    <link>https://dominiquemakowski.github.io/tag/r/</link>
      <atom:link href="https://dominiquemakowski.github.io/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 31 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dominiquemakowski.github.io/media/icon_hu1b87c1310da332ba1b18c001e4f10dac_153312_512x512_fill_lanczos_center_3.png</url>
      <title>R</title>
      <link>https://dominiquemakowski.github.io/tag/r/</link>
    </image>
    
    <item>
      <title>How to sync two folders in two separate GitHub repositories</title>
      <link>https://dominiquemakowski.github.io/post/2021-10-31-sync_two_repos/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2021-10-31-sync_two_repos/</guid>
      <description>&lt;h1 id=&#34;how-to-sync-two-folders-in-two-separate-github-repositories&#34;&gt;How to sync two folders in two separate GitHub repositories&lt;/h1&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;
&lt;p&gt;I have a personal website, stored in a GitHub repo (and hosted via GitHub pages), as well as a lab website (a &amp;ldquo;company&amp;rdquo; website, if you will). Both are fairly similar, as they are built using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;academic theme&lt;/a&gt;. Importantly, there is a blog in the company websites with posts, but I have one on my personal website too. &lt;strong&gt;I would like that every time I post something on my website, that it gets automatically copied over to the company website.&lt;/strong&gt; So that I don&amp;rsquo;t have to manually maintain the content at two separate places.&lt;/p&gt;
&lt;h2 id=&#34;the-solution&#34;&gt;The Solution&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The first step is to go to the settings of your GitHub account, to developers settings, and to &lt;a href=&#34;https://github.com/settings/tokens&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;personal access tokens&lt;/em&gt;&lt;/a&gt;. You have to generate a token, and tick the &lt;strong&gt;repo&lt;/strong&gt; authorizations. Copy-paste the key.&lt;/li&gt;
&lt;li&gt;Go to the settings of the personal website repo (the source from which the content will be copied), to &amp;ldquo;Secrets&amp;rdquo;, and add a new secret called &amp;ldquo;API_TOKEN_GITHUB&amp;rdquo; (with the key you just copied).&lt;/li&gt;
&lt;li&gt;Create a new GitHub action workflow such as &lt;a href=&#34;https://github.com/DominiqueMakowski/DominiqueMakowski.github.io/blob/master/.github/workflows/copy_posts.yml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;this one&lt;/strong&gt;&lt;/a&gt;. The things to change are the &lt;code&gt;source_file&lt;/code&gt;, &lt;code&gt;destination_repo&lt;/code&gt; and &lt;code&gt;destination_folder&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Tada 🎉 Everytime I push to my personal repo, the new content of one of the subfolder gets copied to another repo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; this is a one-way sync, so updates on the target repo won&amp;rsquo;t affect the source repo (but might get overridden!).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to share or tweet this post, or leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>&#34;Your sample is too small&#34;: How to Answer to Reviewers</title>
      <link>https://dominiquemakowski.github.io/post/2021-11-05-sample-too-small/</link>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2021-11-05-sample-too-small/</guid>
      <description>&lt;h1 id=&#34;your-sample-is-too-small-how-to-answer-to-reviewers&#34;&gt;&amp;ldquo;Your sample is too small&amp;rdquo;: How to Answer to Reviewers&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Preregistration,&lt;/li&gt;
&lt;li&gt;Power Analysis&lt;/li&gt;
&lt;li&gt;Registered reports&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to share or tweet this post, or leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to share data analysis scripts with publications?</title>
      <link>https://dominiquemakowski.github.io/post/2021-02-10-template_results/</link>
      <pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2021-02-10-template_results/</guid>
      <description>&lt;h1 id=&#34;how-to-share-data-analysis-scripts-with-publications&#34;&gt;How to share data analysis scripts with publications?&lt;/h1&gt;
&lt;p&gt;Including data analysis as &lt;strong&gt;Supplementary Materials&lt;/strong&gt; can be a tedious task. How can we simplify the sharing of our work? So that it can be fully appreciated, as well as evaluated, improved, worked on, in a transparent and open way?&lt;/p&gt;
&lt;h2 id=&#34;option-1-dump-the-code-in-a-word-file&#34;&gt;Option 1: Dump the code in a word file&lt;/h2&gt;
&lt;p&gt;Most publication portals don&amp;rsquo;t directly accept code scripts to be included &amp;ldquo;as is&amp;rdquo;. In other words, you cannot upload your manuscript and your &lt;code&gt;.R&lt;/code&gt; script just like that. So one option is to copy its content, paste it in a word / pdf document, and &lt;em&gt;Voilà!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But what &lt;strong&gt;if you don&amp;rsquo;t have code&lt;/strong&gt; because you use a point-and-click software? Well, you can note down all the &lt;em&gt;x,y&lt;/em&gt; coordinates of your clicks so that one can reproduce the steps and all the clicks. Just kidding, if you don&amp;rsquo;t have a script, then may god have mercy on your soul.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is that a terrible solution?&lt;/strong&gt; Because let&amp;rsquo;s face it, unstructured code dumps are horrific. Nobody wants to read it, it does not make justice to your work, and it&amp;rsquo;s still tedious to create! You have to re-do it everytime you modify your code. And it&amp;rsquo;s even worse if you want to include all the &lt;strong&gt;outputs, figures, tables that are generated by the code&lt;/strong&gt;? Data analysis is not just the code, but everything that comes with it and that allowed you to make the conclusions that you made.&lt;/p&gt;
&lt;h2 id=&#34;option-2-use-rmarkdown&#34;&gt;Option 2: Use &lt;em&gt;Rmarkdown&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Rmarkdown&lt;/strong&gt;&lt;/a&gt; is a &amp;ldquo;framework&amp;rdquo; that allows you to have files (&lt;code&gt;.Rmd&lt;/code&gt;) that can contain a mix of &lt;strong&gt;text and code&lt;/strong&gt; (and not only &lt;strong&gt;R&lt;/strong&gt;, but also &lt;a href=&#34;https://rstudio.github.io/reticulate/articles/r_markdown.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/a&gt; for instance!).&lt;/p&gt;
&lt;p&gt;It can be used to write comprehensive &amp;ldquo;reports&amp;rdquo; that include all your thoughts, motivations and interpretations of the results. And the great thing about it is that these files can be &lt;strong&gt;converted&lt;/strong&gt; into beautiful and readable documents like &lt;strong&gt;PDF&lt;/strong&gt;, &lt;strong&gt;Word&lt;/strong&gt; or &lt;strong&gt;HTML&lt;/strong&gt;. It will automatically embed the code and &lt;strong&gt;its generated output&lt;/strong&gt; (as text, tables or figures) alongside the text.&lt;/p&gt;
&lt;p&gt;It is an awesome way to write statistical reports, and can even be used to create many other non-stats related stuff, like &lt;a href=&#34;http://frederikaust.com/papaja_man/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;APA-formatted manuscripts&lt;/strong&gt;&lt;/a&gt; (great for preprints), &lt;a href=&#34;https://bookdown.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;books&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/xaringan.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;presentation slides&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;websites or blogs&lt;/strong&gt;&lt;/a&gt;. It&amp;rsquo;s a must-have skill for every researcher.&lt;/p&gt;
&lt;p&gt;And it gets better!&lt;/p&gt;
&lt;h2 id=&#34;option-3-use-our-results-template&#34;&gt;Option 3: Use our &lt;em&gt;Results Template&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;https://dominiquemakowski.github.io/research/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reality Bending&lt;/strong&gt;&lt;/a&gt; team, we like to have our different projects and studies organized in a consistent way. We heavily use &lt;a href=&#34;https://dominiquemakowski.github.io/post/github_psychologists/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt; to store our projects and collaborate on them, and we also like the possibility of making these projects &lt;strong&gt;open&lt;/strong&gt; and &lt;strong&gt;accessible&lt;/strong&gt; (i.e., easy to discover and explore) when the time comes.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we came up with a &lt;strong&gt;Template folder&lt;/strong&gt; for storing the materials related to a given study, including well-organized analysis scripts. And what&amp;rsquo;s great about it is that it is setup in a way that allows you to generate multiple files format (&lt;strong&gt;word&lt;/strong&gt;, &lt;strong&gt;pdf&lt;/strong&gt;, &lt;strong&gt;html&lt;/strong&gt;) with a single click (and even without any click, in a fully automatic way)! And what&amp;rsquo;s even greater is that if you decide to upload it to GitHub, you&amp;rsquo;ll have &lt;a href=&#34;https://realitybending.github.io/TemplateResults/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;a whole website&lt;/strong&gt;&lt;/a&gt; presenting your data analysis!&lt;/p&gt;
&lt;p&gt;We use it to have reproducible analysis that we can easily share with publications. We can upload the .pdf or .docx file generated by the template as &lt;strong&gt;Supplementary Materials&lt;/strong&gt;, but we also link the &lt;a href=&#34;https://github.com/RealityBending/TemplateResults&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;URL of the online repository&lt;/strong&gt;&lt;/a&gt; of the study in the manuscript, where users can access and experience the content in the format that they prefer. &lt;strong&gt;It really improves the appeal of a study when the results are trustworthy.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All this is easily made possible with our template. &lt;strong&gt;Check it out here:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;👉 &lt;a href=&#34;https://github.com/RealityBending/TemplateResults&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;https://github.com/RealityBending/TemplateResults&lt;/strong&gt;&lt;/a&gt; 👈&lt;/p&gt;
&lt;p&gt;☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️☝️&lt;/p&gt;
&lt;p&gt;And &lt;strong&gt;let us know what you think!&lt;/strong&gt; You can open an issue on the &lt;a href=&#34;https://github.com/RealityBending/TemplateResults/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt; or even contribute to help us improve it :)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to tweet and share this post, and leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
&lt;p&gt;🐦 &lt;em&gt;Don&amp;rsquo;t forget to join me on Twitter&lt;/em&gt; &lt;a href=&#34;https://twitter.com/Dom_Makowski&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Dom_Makowski&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to extract individual scores from repeated measures</title>
      <link>https://dominiquemakowski.github.io/post/2020-09-14-individual_scores/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2020-09-14-individual_scores/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Many psychology fields require to extract individual scores, i.e., point-estimates (&lt;em&gt;i.e.&lt;/em&gt;, a single value) for a participant/patient, to be used as an index of something and later interpreted or re-used in further statistical analyses. This single index is often derived from several “trials”. For instance, the reaction times in the condition A (let’s say, the baseline) will be &lt;strong&gt;averaged&lt;/strong&gt; together, and the same will be done with the condition B. Finally, the difference between these two means will be used an the &lt;strong&gt;individual score&lt;/strong&gt; for a given participant.&lt;/p&gt;
&lt;p&gt;However, we can intuitively feel that we &lt;strong&gt;lose a lot of information&lt;/strong&gt; when averaging these scores. Do we deal appropriately with the variability related to individuals, conditions, or the noise aggravated by potential outliers? This is especially important when working with a limited amount of trials.&lt;/p&gt;
&lt;p&gt;With the advent of recent computational advances, new easy-to-implement alternatives emerge. For instance, &lt;strong&gt;one can “model” the effects at an individual level&lt;/strong&gt; (e.g., the simplest case, for the two conditions paradigm described above, would be a linear regression with the condition as a unique predictor), and use the &lt;strong&gt;parameters&lt;/strong&gt; of each model as individual scores (e.g., the “slope” coefficient of the effect of the manipulation), rather than the raw mean. This opens up the possibility of including covariates and take into account other sources of known variability, which could lead to better estimates.&lt;/p&gt;
&lt;p&gt;However, individual models are also sensitive to outliers and noise. Thus, another possibility is to &lt;strong&gt;model the effects at the population level&lt;/strong&gt; and, &lt;em&gt;at the same time&lt;/em&gt;, at the individual level. This can be achieved by modelling the participants as a &lt;strong&gt;random factor in a mixed model&lt;/strong&gt;. In this case, the individual estimates might benefit from the population estimates. In other words, the effects at the population level will “constrain” or “guide” the estimation at an individual level to potentially limit extreme parameters.&lt;/p&gt;
&lt;p&gt;Unfortunately, the above method requires to have all the data at hand, to be able to fit the population model. This is often not the case in on-going acquisition, or in neuropsychological contexts, in which the practitioners simply acquire data for one patient, and have to compute individual scores, without having access to the detailed population data. Thus, an in-between alternative could make use of &lt;strong&gt;Bayesian models&lt;/strong&gt;, in which the population effects (for instance, the mean effect of the condition) could be entered as an informative &lt;strong&gt;prior&lt;/strong&gt; in the individual models to, again, “guide” the estimation at an individual level and hopefully limit the impact of noise or outliers observations.&lt;/p&gt;
&lt;p&gt;In this post, the aim is to compare these 4 methods (basic individual model - equivalent to using the raw mean, population model, individual model with informative priors) in recovering the “true” effects using a simulated dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;div id=&#34;generate-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Generate Data&lt;/h4&gt;
&lt;p&gt;We generate several datasets in which we manipulate the number of participants, in which the score of interest is the effect of a manipulation as compared to a baseline condition. 20 trials per condition will be generated with a known “true” effect (the centre of the distribution from which the data is generated). Gaussian noise of varying standard deviation will be added to create a natural variability (See the functions’ definition below).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(easystats)

data &amp;lt;- get_data(n_participants=1000, n_trials=20)
results &amp;lt;- get_results(data)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;individual.png&#34; alt=&#34;*Example of a dataset containing 20 participants (shown with different colors). As can be seen, we introduced modulations in the inter- and intra- individual variability.*&#34; width=&#34;1575&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;em&gt;Example of a dataset containing 20 participants (shown with different colors). As can be seen, we introduced modulations in the inter- and intra- individual variability.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We will then compare the scores obtained by each method to the “true” score of each participant by substracting them from one another. As such, for each method, we obtain the absolute “distance” from the true score.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit model&lt;/h4&gt;
&lt;p&gt;Contrast analysis will be applied to compare the different methods together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- lm(Diff_Abs ~ Method, data=results)

modelbased::estimate_contrasts(model) %&amp;gt;%
  arrange(Difference) %&amp;gt;%
  mutate(Level1 = stringr::str_remove(Level1, &amp;quot;Diff_&amp;quot;),
         Level2 = stringr::str_remove(Level2, &amp;quot;Diff_&amp;quot;)) %&amp;gt;% 
  select(Level1, Level2, Difference, CI_low, CI_high, p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Level1                 |                 Level2 | Difference |            CI |      p
## -------------------------------------------------------------------------------------
## IndividualModel_Priors |        PopulationModel |  -1.85e-03 | [-0.01, 0.01] | &amp;gt; .999
## IndividualModel_Freq   |        PopulationModel |   1.70e-03 | [-0.01, 0.01] | &amp;gt; .999
## IndividualModel_Freq   | IndividualModel_Priors |   3.55e-03 | [-0.01, 0.01] | &amp;gt; .999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-the-results&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Visualize the results&lt;/h4&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;*Average accuracy of the different methods (the closest to 0 the better).*&#34; width=&#34;2250&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;em&gt;Average accuracy of the different methods (the closest to 0 the better).&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;n_participants.png&#34; alt=&#34;*Accuracy depending on the number of total participants in the dataset.*&#34; width=&#34;2250&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: &lt;em&gt;Accuracy depending on the number of total participants in the dataset.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Though not significantly different, it seems that &lt;strong&gt;raw basic estimates&lt;/strong&gt; (that rely only on the individual data) &lt;strong&gt;perform consistently worse than the population model or individual models informed by priors&lt;/strong&gt;, especially for small datasets (between 10 and 100 participants) - though again, the difference is tiny in our simulated dataset. In the absence of the whole population dataset, it seems that using individual Bayesian model with informative priors (derived from the population model) is a safe alternative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(easystats)
library(rstanarm)
library(ggdist)


# Get data ----------------------------------------------------------------

get_data &amp;lt;- function(n_participants = 10, n_trials = 20, d = 1.5, var = 3, noise = 0.1) {
  scores_baseline &amp;lt;- rnorm(n_participants, 0, 1)
  scores_condition &amp;lt;- rnorm(n_participants, d, 1)
  variances &amp;lt;- rbeta(n_participants, 2, 8)
  variances &amp;lt;- 0.1 + variances * (var / max(variances)) # Rescale
  noise_sd &amp;lt;- abs(rnorm(n_participants, 0, noise))

  data &amp;lt;- data.frame()
  for (i in 1:n_participants) {
    a &amp;lt;- rnorm(n_trials, scores_baseline[i], variances[i])
    b &amp;lt;- rnorm(n_trials, scores_condition[i], variances[i])
    a &amp;lt;- a + rnorm(n_trials, 0, noise_sd[i]) # Add noise
    b &amp;lt;- b + rnorm(n_trials, 0, noise_sd[i]) # Add noise
    data &amp;lt;- rbind(data, data.frame(
      &amp;quot;Participant&amp;quot; = sprintf(&amp;quot;S%02d&amp;quot;, i),
      &amp;quot;Y&amp;quot; = c(a, b),
      &amp;quot;Score_True&amp;quot; = rep(c(scores_baseline[i], scores_condition[i]), each = n_trials),
      &amp;quot;Condition&amp;quot; = rep(c(&amp;quot;Baseline&amp;quot;, &amp;quot;Manipulation&amp;quot;), each = n_trials)
    ))
  }
  data
}



# Visualize data -----------------------------------------------------------

p &amp;lt;- get_data(n_participants = 20) %&amp;gt;%
  group_by(Participant, Condition) %&amp;gt;%
  mutate(mean = mean(Y)) %&amp;gt;%
  ggplot(aes(y = Y, x = Condition, fill = Participant, color = Participant, group = Participant)) +
  geom_line(aes(y = mean), position = position_dodge(width = 0.66)) +
  ggdist::stat_eye(point_interval = ggdist::mean_hdi, alpha = 0.66, position = position_dodge(width = 0.66), .width = c(0.95)) +
  ylab(&amp;quot;Score&amp;quot;) +
  theme_modern() +
  theme(legend.position = &amp;quot;none&amp;quot;)
ggsave(&amp;quot;individual.png&amp;quot;, p, width = 7, height = 7, dpi = 450)


# Get results -------------------------------------------------------------


get_results &amp;lt;- function(data) {

  # Raw method ----

  results &amp;lt;- data %&amp;gt;%
    group_by(Participant, Condition) %&amp;gt;%
    summarise_all(mean) %&amp;gt;%
    rename(&amp;quot;Score_Raw&amp;quot; = &amp;quot;Y&amp;quot;) %&amp;gt;%
    arrange(Condition, Participant) %&amp;gt;%
    ungroup()


  # Population model ----

  model &amp;lt;- lme4::lmer(Y ~ Condition + (1 + Condition | Participant), data = data)

  fixed &amp;lt;- insight::get_parameters(model, effects = &amp;quot;fixed&amp;quot;)$Estimate
  random &amp;lt;- insight::get_parameters(model, effects = &amp;quot;random&amp;quot;)$Participant

  # Transform coefs into scores
  pop_baseline &amp;lt;- random[, 1] + fixed[1]
  pop_manipulation &amp;lt;- pop_baseline + random[, 2] + fixed[2]

  results$Score_PopulationModel &amp;lt;- c(pop_baseline, pop_manipulation)


  # Individual model ----

  individual_model_data &amp;lt;- data.frame()
  for (participant in unique(data$Participant)) {
    cat(&amp;quot;.&amp;quot;) # Print progress

    dat &amp;lt;- data[data$Participant == participant, ]

    # Frequentist
    model1 &amp;lt;- lm(Y ~ Condition, data = dat)
    nopriors &amp;lt;- parameters::parameters(model1)$Coefficient

    # Bayesian without priors
    # model2 &amp;lt;- stan_glm(Y ~ Condition, data = dat, refresh = 0)
    # bayes &amp;lt;- parameters::parameters(model2)$Median

    # Bayesian with Priors
    model3 &amp;lt;- stan_glm(Y ~ Condition,
      data = dat,
      refresh = 0,
      prior = normal(fixed[1]),
      prior_intercept = normal(fixed[2])
    )
    priors &amp;lt;- parameters::parameters(model3)$Median

    individual_model_data &amp;lt;- rbind(
      individual_model_data,
      data.frame(
        &amp;quot;Participant&amp;quot; = c(participant, participant),
        &amp;quot;Condition&amp;quot; = c(&amp;quot;Baseline&amp;quot;, &amp;quot;Manipulation&amp;quot;),
        &amp;quot;Score_IndividualModel&amp;quot; = c(nopriors[1], nopriors[1] + nopriors[2]),
        # &amp;quot;Score_IndividualModel_Bayes&amp;quot; = c(bayes[1], bayes[1] + bayes[2]),
        &amp;quot;Score_IndividualModel_Priors&amp;quot; = c(priors[1], priors[1] + priors[2])
      )
    )
  }

  results &amp;lt;- merge(results, individual_model_data)


  # Clean output ----

  diff &amp;lt;- results %&amp;gt;%
    mutate(
      # Diff_Raw = Score_True - Score_Raw,
      Diff_PopulationModel = Score_True - Score_PopulationModel,
      Diff_IndividualModel = Score_True - Score_IndividualModel,
      # Diff_IndividualModel_Bayes = Score_True - Score_IndividualModel_Bayes,
      Diff_IndividualModel_Priors = Score_True - Score_IndividualModel_Priors
    ) %&amp;gt;%
    select(Participant, Condition, starts_with(&amp;quot;Diff&amp;quot;)) %&amp;gt;%
    pivot_longer(starts_with(&amp;quot;Diff&amp;quot;), names_to = &amp;quot;Method&amp;quot;, values_to = &amp;quot;Diff&amp;quot;) %&amp;gt;%
    mutate(Diff_Abs = abs(Diff))

  diff
}



# Analysis ----------------------------------------------------------------
results &amp;lt;- data.frame()
for(n in seq.int(10, 300, length.out = 10)){
  data &amp;lt;- get_data(n_participants = round(n), n_trials = 20)
  rez &amp;lt;- get_results(data) %&amp;gt;%
    select(-Participant) %&amp;gt;%
    group_by(Condition, Method) %&amp;gt;%
    summarise_all(mean) %&amp;gt;%
    mutate(n_Participants = n,
           Method = as.factor(Method),
           Dataset=paste0(&amp;quot;Dataset&amp;quot;, round(n, 2)))
  results &amp;lt;- rbind(results, rez)

  print(n) # Print progress
}

# model &amp;lt;- mgcv::gam(Diff_Abs ~ Method + s(n_Participants, by = Method), data = results)
model &amp;lt;- lm(Diff_Abs ~ Method * poly(n_Participants, 3), data = results)

parameters::parameters(model)


contrasts &amp;lt;- modelbased::estimate_contrasts(model) %&amp;gt;%
  arrange(Difference) %&amp;gt;%
  mutate(
    Level1 = stringr::str_remove(Level1, &amp;quot;Diff_&amp;quot;),
    Level2 = stringr::str_remove(Level2, &amp;quot;Diff_&amp;quot;)
  ) %&amp;gt;%
  select(Level1, Level2, Difference, CI_low, CI_high, p)




# Visualize results ---------------------------------------------------------
p &amp;lt;- modelbased::estimate_means(model) %&amp;gt;%
  arrange(Mean) %&amp;gt;%
  mutate(
    Method = stringr::str_remove(Method, &amp;quot;Diff_&amp;quot;),
    Method = factor(Method, levels = Method)
  )  %&amp;gt;%
  ggplot(aes(x = Method, y = Mean, color = Method)) +
  geom_line(aes(group = 1)) +
  geom_pointrange(aes(ymin = CI_low, ymax = CI_high), size = 1) +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_material()
ggsave(&amp;quot;featured.png&amp;quot;, p, width = 10, height = 6, dpi = 450)



p &amp;lt;- modelbased::estimate_relation(model) %&amp;gt;%
  mutate(Method = stringr::str_remove(Method, &amp;quot;Diff_&amp;quot;))  %&amp;gt;%
  ggplot(aes(x = n_Participants, y = Predicted)) +
  geom_point(data=mutate(results, Method = stringr::str_remove(Method, &amp;quot;Diff_&amp;quot;)),
             aes(y=Diff_Abs, color = Method)) +
  geom_ribbon(aes(ymin=CI_low, ymax=CI_high, fill=Method), alpha=0.1) +
  geom_line(aes(color = Method), size=1) +
  theme_modern() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_material() +
  scale_fill_material()
ggsave(&amp;quot;n_participants.png&amp;quot;, p, width = 10, height = 6, dpi = 450)


# Save results ------------------------------------------------------------


d &amp;lt;- list(&amp;quot;results&amp;quot; = results, &amp;quot;model&amp;quot; = model, &amp;quot;contrasts&amp;quot; = contrasts)
save(d, file = &amp;quot;data.Rdata&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;sub&gt;You can reference this post as follows:&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;&lt;sub&gt;- Makowski, D. (2020, September 14). How to extract individual scores from repeated measures. Retrieved from &lt;a href=&#34;https://dominiquemakowski.github.io/post/individual_scores/&#34; class=&#34;uri&#34;&gt;https://dominiquemakowski.github.io/post/individual_scores/&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to share this post, and leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
&lt;p&gt;🐦 &lt;em&gt;And don’t forget to join me on Twitter&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;[@Dom_Makowski]&lt;/span&gt;(&lt;a href=&#34;https://twitter.com/Dom_Makowski&#34; class=&#34;uri&#34;&gt;https://twitter.com/Dom_Makowski&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R or Python for Psychologists</title>
      <link>https://dominiquemakowski.github.io/post/2020-05-22-r_or_python/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2020-05-22-r_or_python/</guid>
      <description>&lt;p&gt;Many psychology students or researchers are faced with the challenge - &lt;em&gt;or the opportunity&lt;/em&gt; - of learning a programming language. &lt;strong&gt;Which one should you learn?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As an ex- psych student and a daily user and developer of both, here&amp;rsquo;s my take on this hot debate.&lt;/p&gt;
&lt;h2 id=&#34;what-has-programming-to-do-with-psychology&#34;&gt;What has programming to do with psychology?&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re a very young psychology student, or a future one, you might wonder: &lt;strong&gt;why the heck would I have to learn programming in psychology?&lt;/strong&gt; &lt;em&gt;&amp;ldquo;Psychology is like philosophy, it&amp;rsquo;s just learning how people&amp;rsquo;s minds work by reading books and overthinking stuff&amp;rdquo;&lt;/em&gt;. If you still think that, you&amp;rsquo;re in for &lt;strong&gt;one hell of a ride&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Psychology is, since its very beginning, a hard and experimental science. The founding fathers of psychology were dedicated to find ways to objectively &lt;em&gt;measure&lt;/em&gt; psychological phenomena and uncovering the mathematical laws that govern Human behaviour (one of the fields of psychology is even called psycho&lt;em&gt;physics&lt;/em&gt;). This &lt;em&gt;sciency&lt;/em&gt; nature has been toned down by the booming popularity of &lt;strong&gt;pseudo-scientific approaches like psychoanalysis&lt;/strong&gt;, that contributed to the stereotypical public image of the shrink doodling while listening to a neurotic patient. But that&amp;rsquo;s a distorted and old-fashioned view, clearly not representative of the future of psychology.&lt;/p&gt;
&lt;p&gt;The fact is that psychology is very closely connected with &lt;strong&gt;statistics&lt;/strong&gt;. Many great statistical advances were made by psychologists, and all true psychological discoveries are backed by statistical findings. And this importance of statistics is - and will be - growing further, partly due to the recent realization of some major issues in the field due to improper statistical procedures (coined the &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_crisis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;replicability crisis&lt;/strong&gt;&lt;/a&gt;&amp;quot;). Moreover, psychology is more and more relying on advanced data-acquiring methods (smartphone apps, website data, physiological and brain recording devices like EEG, MRI, etc.). And these new formats often require specialized knowledge (web-scraping, database querying, neuroimaging, signal processing, machine learning, &amp;hellip;). Even in the most applied kind of &lt;strong&gt;clinical&lt;/strong&gt; or &lt;strong&gt;psychotherapeutic&lt;/strong&gt; specialization, where you&amp;rsquo;d think you&amp;rsquo;d be safe, they are starting to use data intensive methods like neuro-feedback, virtual reality or smartphone sensing and surveying.&lt;/p&gt;
&lt;p&gt;Long story short, no matter which branch of psychology you specialize in, you &lt;em&gt;will&lt;/em&gt; be confronted with some technical aspects that won&amp;rsquo;t be able to solve with &lt;em&gt;Excel&lt;/em&gt;. Importantly, these are the skills that will make the most difference between students, and that will matter a lot if you want to pursue research.&lt;/p&gt;
&lt;p&gt;So, &lt;strong&gt;ready to dive into programming?&lt;/strong&gt; Fear not! It&amp;rsquo;s not that complicated. Moreover, it&amp;rsquo;s &lt;strong&gt;one of the most rewarding skill&lt;/strong&gt; you can develop. I can assure you that you won&amp;rsquo;t regret the time invested in learning it 😊&lt;/p&gt;
&lt;p&gt;But where should you start?&lt;/p&gt;
&lt;h2 id=&#34;both-r-and-python&#34;&gt;Both R and Python&lt;/h2&gt;
&lt;p&gt;This increasing relationship between psychology and statistics on the one hand, and other more general technical aspects on the other, is the reason why R and Python are so popular in psychology. Both languages are &lt;strong&gt;free&lt;/strong&gt;, &lt;strong&gt;open-source&lt;/strong&gt;, suited for &lt;strong&gt;beginners&lt;/strong&gt;, and have a large base of users with a ton of &lt;strong&gt;learning material&lt;/strong&gt; online. What&amp;rsquo;s the difference between them?&lt;/p&gt;
&lt;p&gt;Put simply, &lt;strong&gt;R is for statistics, Python is for the rest&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So why is there a virulent debate going on, and a choice to make? It&amp;rsquo;s true that I, &lt;em&gt;in theory&lt;/em&gt;, would recommend &lt;strong&gt;learning both&lt;/strong&gt;, as they are complementary and have their own strengths and weaknesses.&lt;/p&gt;
&lt;p&gt;However, many opinionated people arguing in favour of one &lt;strong&gt;or&lt;/strong&gt; the other (usually the only one they know) will say that learning both is essentially a waste of time. They will put forth a strong argument: &lt;strong&gt;you can do whatever you do in R in Python, and &lt;em&gt;vice-versa&lt;/em&gt;&lt;/strong&gt;. In other words, both languages can be used to achieve your goals, and it&amp;rsquo;s &lt;strong&gt;better to specialize in one than have a limited knowledge of both&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Although I do not agree with that statement, I do acknowledge that people have limited time and resources to learn. Saying &lt;strong&gt;&amp;ldquo;just learn both&amp;rdquo;&lt;/strong&gt; is easy, but is arguably an unrealistic expectation for the vast majority of people. So why learning both can be a long-term goal (especially if you want to do research), you have to start somewhere. So, &lt;strong&gt;what starter language should you select?&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;pokemon.png&#34; alt=&#34;r or python&#34;/&gt;
  &lt;figcaption&gt;Ash choosing his starter programming language. He has the choice between R, Python and Bulbasaur, i.e, Matlab - the one that no one likes.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;what-about-matlab&#34;&gt;What about Matlab?&lt;/h2&gt;
&lt;p&gt;There was a time when &lt;em&gt;Matlab&lt;/em&gt; was the boss. It was used everywhere and had the best functionalities for neuroimaging, signal processing and maths. But &lt;strong&gt;that time is over&lt;/strong&gt;. Matlab is already a dead language, which burial process will continue in the years to come. Why is it dead? Because it is &lt;strong&gt;expensive&lt;/strong&gt;, &lt;strong&gt;closed&lt;/strong&gt;, &lt;strong&gt;ugly&lt;/strong&gt;, and most importantly the alternatives (namely R and Python) are now more powerful and featured than Matlab.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;https://media.giphy.com/media/sDOhzJBsFvjMY/giphy.gif&#34; alt=&#34;matlab&#34;/&gt;
  &lt;figcaption&gt;Agamemnon reacting to king Priam saying &#34;The city of Matlab will never be conquered&#34;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The truth is, there are only two reasons people still use Matlab: &lt;strong&gt;habit&lt;/strong&gt; (it&amp;rsquo;s hard to learn a new approach if your old way of doing things still works) and &lt;strong&gt;SPM&lt;/strong&gt; (a toolbox for neuroimaging that is still - &lt;em&gt;for now&lt;/em&gt; - the leader in the field).&lt;/p&gt;
&lt;p&gt;But seriously, don&amp;rsquo;t waste time on it if you have limited resources, it&amp;rsquo;s just not worth it. You will learn an outdated tool that you won&amp;rsquo;t be able to use in another lab if they don&amp;rsquo;t agree to pay for an expensive license (unless you&amp;rsquo;re a pirate ☠️). Whereas with open and free languages like R or Python, you have access to the best tools and can use them freely everywhere. Also, it makes you a &lt;strong&gt;supporter of open-science&lt;/strong&gt;, and that&amp;rsquo;s trendy 😁.&lt;/p&gt;
&lt;h2 id=&#34;how-to-decide-between-r-and-python&#34;&gt;How to decide between R and Python&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Time has come to make a decision.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Despite what people say, &lt;strong&gt;R and Python are not equivalent&lt;/strong&gt;. You can argue as much as you want, but doing statistics and data visualization in Python is not as fast, easy and neat as it is in R. And signal processing or neuroimaging is not as powerful in R as compared to Python. Note that both languages are still growing and changing, and they are influencing themselves: for instance, many popular Python modules (e.g., &lt;strong&gt;pandas&lt;/strong&gt;, &lt;strong&gt;statsmodels&lt;/strong&gt;, &lt;strong&gt;seaborn&lt;/strong&gt;, &amp;hellip;) have been directly inspired by R. As such, the boundaries between the two languages are fading (and I&amp;rsquo;m not even mentioning the great advances in interoperability, with tools like &lt;a href=&#34;https://rstudio.github.io/reticulate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;reticulate&lt;/strong&gt;&lt;/a&gt; that allow you to use one language directly inside the other).&lt;/p&gt;
&lt;p&gt;That being said, Python and R remain very different languages at their core, with a different feel and vibe to it. R was made by statisticians for statistics, and the majority of its users are academics and researchers. On the contrary, Python is a true general-purpose &amp;ldquo;programming&amp;rdquo; language, widely used outside of academia, in the private sector.&lt;/p&gt;
&lt;p&gt;Here are some things to consider when deciding on what language to learn:&lt;/p&gt;
&lt;h3 id=&#34;reasons-to-choose-python&#34;&gt;Reasons to choose Python&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You have some basic knowledge or familiarity with programming&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, you know what a loop is. Python being a true programming language, having any prior experience will be useful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are good with logic and spatial representation (like imagining shapes in 3D, rotating them, etc.)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Python, you will have to think with a &amp;ldquo;programming&amp;rdquo; mindset. That means perceiving things in terms of logical statements and blocks, understanding data as 2D or 3D tables that you have to slice and recombine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are comfortable with maths&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Python, numbers and numbers combinations are used a lot. Paradoxically, you will typically see a lot more of maths in Python than in R.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You plan to do signal processing or experimental tasks creation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are some of the domains where Python is well-established (which doesn&amp;rsquo;t mean that R doesn&amp;rsquo;t have some great tools in development).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are good at googling and don&amp;rsquo;t mind spending time looking for the right answer&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python has so much material online that it&amp;rsquo;s sometimes hard to find the appropriate thing. Harder than in R, in my opinion.&lt;/p&gt;
&lt;h3 id=&#34;reasons-to-choose-r&#34;&gt;Reasons to choose R&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You have no experience with programming whatsoever&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R is not made to be used as a traditional &lt;em&gt;programming&lt;/em&gt; language. It&amp;rsquo;s more of finding what functions to apply to what, and that makes it easy for beginners.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are interested in statistical analyses, modelling things, and making inferences from data&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R excels at this. You can create powerful models super easily and jump into their understanding and interpretation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You like making nice figures and plots&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R, through the &lt;a href=&#34;https://ggplot2.tidyverse.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;ggplot&lt;/strong&gt;&lt;/a&gt; ecosystem, has hands down the best tools for visualization. Your imagination is the limit (check-out the artworks by &lt;a href=&#34;https://www.data-imaginist.com/art&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Lin Pedersen&lt;/a&gt; abd &lt;a href=&#34;https://art.djnavarro.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Danielle Navarro&lt;/a&gt; 😍).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are &lt;em&gt;not&lt;/em&gt; so good with stats or maths&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You heard it right! To start with R you don&amp;rsquo;t need to know stats or maths like a boss. R, in fact, will help you to become proficient at it, by slowly opening more and more layers of complexity to you, if you are deemed worthy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You are interested in joining the academic community&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because most of its users are academics, R has a fantastic community online, for instance on &lt;a href=&#34;https://twitter.com/hashtag/rstats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Twitter #rstats&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;other-considerations&#34;&gt;Other considerations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What your peers are learning&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s easier to learn together, so try to discuss it with your class or lab mates if you can.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;What your lab is using&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It might be easier if you have mentors that can understand what you are doing and guide you. But that should not be a priority, as it can lead to old habits reproduction (especially if your lab has a tradition of Matlab 🤭).&lt;/p&gt;
&lt;h2 id=&#34;hands-on&#34;&gt;Hands on!&lt;/h2&gt;
&lt;p&gt;👉 Looking for places to start? Check out this &lt;a href=&#34;https://neurokit2.readthedocs.io/en/latest/tutorials/learnpython.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;10-min crash course introduction to Python&lt;/strong&gt;&lt;/a&gt; and this &lt;a href=&#34;https://easystats.github.io/blog/resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;collection of resources for R&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt; 🐦 &lt;em&gt;Don&amp;rsquo;t forget to join me on Twitter&lt;/em&gt; (&lt;a href=&#34;https://twitter.com/Dom_Makowski&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Dom_Makowski&lt;/a&gt;) &lt;em&gt;and leave a comment below&lt;/em&gt; 👇&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to correctly analyze reaction time (RT) data</title>
      <link>https://dominiquemakowski.github.io/post/2020-05-18-analyze_rt/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2020-05-18-analyze_rt/</guid>
      <description>&lt;p&gt;This is a very, very important topic given the widespread usage of reaction times in psychology. Most of the time, we analyze it as a regular variable, using traditional models such as &lt;em&gt;linear models&lt;/em&gt;, &lt;em&gt;ANOVAs&lt;/em&gt; etc. The problem is that these models &lt;strong&gt;assume that the RT is normally distributed, which is false&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This leads us to adjustements like &lt;strong&gt;outliers removal&lt;/strong&gt; or &lt;strong&gt;log-transformation&lt;/strong&gt;, distorting the data because of our non-appropriate models.&lt;/p&gt;
&lt;p&gt;The good news is, it&amp;rsquo;s very easy to use better models, that account for the non-normal distribution of RT. And these alternatives are beautifully presented by &lt;a href=&#34;https://vbn.aau.dk/da/persons/117060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas K. Lindeløv&lt;/a&gt; in the guide below:&lt;/p&gt;
&lt;p&gt;👉 &lt;a href=&#34;https://lindeloev.github.io/shiny-rt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reaction time distributions: an interactive overview&lt;/strong&gt;&lt;/a&gt; 👈&lt;/p&gt;
&lt;p&gt;It is a must-read for all psychologist. Do check-it out!!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to tweet and share this post, and leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
&lt;p&gt;🐦 &lt;em&gt;And don&amp;rsquo;t forget to join me on Twitter&lt;/em&gt; &lt;a href=&#34;https://twitter.com/Dom_Makowski&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Dom_Makowski&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian statistics with R</title>
      <link>https://dominiquemakowski.github.io/post/2020-05-14-intro_bayestestr/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/post/2020-05-14-intro_bayestestr/</guid>
      <description>&lt;p&gt;You are a student or a researcher interested in Bayesian statistics and R? But all the tutorials and courses that you have found are too intimidating?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fear no more!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With the &lt;a href=&#34;https://github.com/easystats/easystats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easystats team&lt;/a&gt;, we have created a very &lt;strong&gt;gentle&lt;/strong&gt; and &lt;strong&gt;introductory&lt;/strong&gt; course for beginners.&lt;/p&gt;
&lt;p&gt;You can find the link here:&lt;/p&gt;
&lt;p&gt;👉 &lt;a href=&#34;https://easystats.github.io/bayestestR/articles/bayestestR.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Get Started with Bayesian Statistics using R&lt;/strong&gt;&lt;/a&gt; 👈&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! Do not hesitate to tweet and share this post, and leave a comment below&lt;/em&gt; 🤗&lt;/p&gt;
&lt;p&gt;🐦 &lt;em&gt;Don&amp;rsquo;t forget to join me on Twitter&lt;/em&gt; &lt;a href=&#34;https://twitter.com/Dom_Makowski&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Dom_Makowski&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>easystats</title>
      <link>https://dominiquemakowski.github.io/project/easystats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/project/easystats/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Patient Assessment</title>
      <link>https://dominiquemakowski.github.io/project/patientassessmentapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dominiquemakowski.github.io/project/patientassessmentapp/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
